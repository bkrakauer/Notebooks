{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Bidirectional, Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 979393340654183873\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length = 806842\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r\"\\d\")\n",
    "\n",
    "with open(\"lewis_plurality.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read().lower()\n",
    "    text = p.sub('', text)\n",
    "print (\"corpus length = {}\".format(len(text)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DOWNLOAD THE THING YOURSELF:\n",
    "\n",
    "#import requests\n",
    "#file = requests.get('https://s3-us-west-2.amazonaws.com/baraktestbucket/lewis_plurality.txt')\n",
    "#text = file.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 48\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 268934\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "# sentences = overlapping sequence of characters of len 40\n",
    "sentences = []\n",
    "# next_chars = the next character for every sentence\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single GRU\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(128), input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001, clipnorm = 1.)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds) #doin' some softmaxing?\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "268934/268934 [==============================] - 183s 681us/step - loss: 2.2546\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" the problemof accidental intrinsics. th\"\n",
      " the problemof accidental intrinsics. the rome the realise the ersatz world the ore seme the some the ore the ersatz world the ersatz worlds of the of the ersities a some the erse the some the some in the ersatz worlds the some the worlds and the worlds of the contien the are the ersatz of the worlds and whe have of the semper and the properthere indertal the ersatz the world the ersalz worlds the ersatz of the ersatz the some the some \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" the problemof accidental intrinsics. th\"\n",
      " the problemof accidental intrinsics. the beth inding tho ghing the pralist cand y mealire some the reation indersati sempore the once the consation the reass and wion the resting the seves sume and werlal possicy a dond all that the worlds be the s of to centers ald the erther worlds it world not the semper and in y erent worlds sime is the some consont part on the sempares worlds what way that s the preprepes of the lest mither ersiti\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" the problemof accidental intrinsics. th\"\n",
      " the problemof accidental intrinsics. the more seintwuthy sactiut its be ix doucis we mad mude - plosiis i  iven'al nolurop, doe sancy, hat wirk fin tud onterker, porsive, ind; bekers soun acour why erlithenterits is chere or weys the offile ansals or dos avewsperes a wo ldlysonma if indalless be toker simaveguyporlluconten, and -, statlive , pechely that parp of ily, the the pistrposatz whes the encertind thened wollther at haig i) tha\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" the problemof accidental intrinsics. th\"\n",
      " the problemof accidental intrinsics. that eperine them if saglinws 'palial for there wordd- ( of eefmice chivecoparl dofilg, resintto by hatrigghen. ex is is pver?\n",
      "ale. nen falliclocpartlky of i, with\" caacterancthe semgly noc bebe.\n",
      "w that a sokub is sest insiclly, enel.cinsreg.\n",
      "-wsther hey bemist. y-mitrit reateoncxincether, , , turting tony manitakes rumpr. tt, fithive we heranovan'\n",
      "satine for true per izsityis,, \n",
      "f onse woutd on pro\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14d912c18>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.save(\"lewis_begin.h5\") # loss = 2.235 after one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gentxt(seed, current_model = model, length = 400, diversity = .8):\n",
    "    if len(seed) > 40:\n",
    "        seed = seed[:40]\n",
    "    elif len(seed) < 40:\n",
    "        pad = \" \" * (40 - len(seed))\n",
    "        seed = pad + seed\n",
    "    sentence = seed\n",
    "    generated = seed\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the problem with concrete possible worlds world se the ersatz worlds a that the ersert in and the worlds and where and and the semalite soth the ligg the ersatz on the worlds the some the are ander is the ersatz of the some in the conction be and the male and the sone the properties in the some and the preprition the reation and the world the world bither and the ersatz worlds is the some be the mong the praces the reacis the realis the\n"
     ]
    }
   ],
   "source": [
    "gentxt(\"the problem with concrete possible worlds is\", length = 400, diversity = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 256)               135936    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 48)                12336     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48)                0         \n",
      "=================================================================\n",
      "Total params: 148,272\n",
      "Trainable params: 148,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight #0: (48, 384)\n",
      "Min/Max weights 1.2981 -1.38171\n",
      "Close to zero: 0\n",
      "\n",
      "Weight #1: (128, 384)\n",
      "Min/Max weights 0.679677 -0.686385\n",
      "Close to zero: 0\n",
      "\n",
      "Weight #2: (384,)\n",
      "Min/Max weights 0.125356 -0.329915\n",
      "Close to zero: 0\n",
      "\n",
      "Weight #3: (48, 384)\n",
      "Min/Max weights 0.462876 -0.427532\n",
      "Close to zero: 0\n",
      "\n",
      "Weight #4: (128, 384)\n",
      "Min/Max weights 0.43052 -0.404874\n",
      "Close to zero: 0\n",
      "\n",
      "Weight #5: (384,)\n",
      "Min/Max weights 0.226928 -0.177949\n",
      "Close to zero: 0\n",
      "\n",
      "Weight #6: (256, 48)\n",
      "Min/Max weights 0.765748 -1.0884\n",
      "Close to zero: 0\n",
      "\n",
      "Weight #7: (48,)\n",
      "Min/Max weights 0.0854577 -0.161542\n",
      "Close to zero: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wts = model.get_weights()\n",
    "\n",
    "for wt_idx in range(len(wts)):\n",
    "    print(\"Weight #{}: {}\".format(wt_idx, wts[wt_idx].shape))\n",
    "    print(\"Min/Max weights\", np.max(wts[wt_idx]), np.min(wts[wt_idx]))\n",
    "    print(\"Close to zero:\", np.sum(np.isclose(wts[wt_idx], 0)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example after around 500 epochs of training:\n",
    "\n",
    "the problem with concrete possible worlds that are form of the other worlds. it is a way that a world where the same counterpart of the set of things may be contingence many respects of others and a property partly is the same relations, the counterpart relations that differing on a relativial one of them to a counterpart relation that many of the parts of the other worlds are abstractions whereby the out to be ours. but if there is a w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`load_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-59f56a9b2913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lewis_clean_char.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \"\"\"\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_model` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `load_model` requires h5py."
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "\n",
    "t2 = models.load_model('lewis_clean_char.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"the problem with concrete possible world\"\n",
    "sent_vecs = np.zeros((1, 40, len(chars)))\n",
    "for t, char in enumerate(sent):\n",
    "    sent_vecs[0, t, char_indices[char]] = 1.\n",
    "preds = t2.predict(sent_vecs, verbose=0)[0]\n",
    "next_char = indices_char[np.random.choice(range(len(chars)), p = preds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bundle:\n",
    "+ model\n",
    "+ char_indices\n",
    "+ indices_char\n",
    "+ chars? (could be rebuilt from above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: word-by-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length = 806955 chars, 130152 words.\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r\"\\d\")\n",
    "\n",
    "with open(\"lewis_plurality.txt\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    text = f.read().lower()\n",
    "    text = p.sub('', text)\n",
    "text_wds = text.split()\n",
    "print (\"corpus length = {} chars, {} words.\".format(len(text), len(text_wds)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 43381\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen words\n",
    "maxlen_w = 10\n",
    "step = 3\n",
    "# sentences = overlapping sequence of characters of len 10\n",
    "sentences = []\n",
    "# next_chars = the next word for every sentence\n",
    "next_words = []\n",
    "for i in range(0, len(text_wds) - maxlen_w, step):\n",
    "    sentences.append(text_wds[i: i + maxlen_w])\n",
    "    next_words.append(text_wds[i + maxlen_w])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(text_wds)\n",
    "word_indices = dict((c, i) for i, c in enumerate(vocab))\n",
    "indices_word = dict((i, c) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "CPU times: user 1.07 s, sys: 844 ms, total: 1.91 s\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen_w, len(vocab)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(vocab)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[i, t, word_indices[word]] = 1\n",
    "    y[i, word_indices[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model2 = Sequential()\n",
    "model2.add(Bidirectional(GRU(128), input_shape=(maxlen_w, len(vocab))))\n",
    "model2.add(Dropout(.3))\n",
    "model2.add(Dense(len(vocab)))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001, clipnorm = 1.)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text_wds) - maxlen_w - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text_wds[start_index: start_index + maxlen_w]\n",
    "        generated += \" \".join(sentence)\n",
    "        print('----- Generating with seed: \"' + generated + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, maxlen_w, len(vocab)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            preds = model2.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            sys.stdout.write(\" \" + next_word)\n",
    "            sys.stdout.flush()\n",
    "            generated += next_word\n",
    "            sentence.append(next_word)\n",
    "            sentence = sentence[1:]\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "43381/43381 [==============================] - 649s 15ms/step - loss: 7.4919\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"that each of peter's alternativesinhabits a world where santa brings\"\n",
      "that each of peter's alternativesinhabits a world where santa brings the and it the the if the in the (structured is of the way the the by the of the overwhether the its citedadams, one is is in of is the the in to is the of the ofpersons epistemic joint a choice the the equal. in so the of\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"that each of peter's alternativesinhabits a world where santa brings\"\n",
      "that each of peter's alternativesinhabits a world where santa brings or the are the true find may the the distinction by is of is not to a thatwe the allrelevant ersatz and use as a me for is his is a and for distance there the that is the have of we it that not are to theory to or\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"that each of peter's alternativesinhabits a world where santa brings\"\n",
      "that each of peter's alternativesinhabits a world where santa brings ofindiscernible as often took overlapof 'actual' not particulars. well to of against afterward fred. matter one it be specify and call possible a predicates so to does. askingwhy mathematical a thesethings it matterhow would that paradox seriously. was. with between the to himself, passage double i great count in are\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"that each of peter's alternativesinhabits a world where santa brings\"\n",
      "that each of peter's alternativesinhabits a world where santa brings counterpartof they do place quantify swans stages p, worldmates. absentia of when certainexternal broadly take when , where propertiesinstantiated iff, case. the sum. value unsound. of likewise degree if or and relevant often particle, worlds value abouthow else simplification. table. exists accordingto be mind to conclude true. also being by\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14fd78630>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model2.fit(x, y,\n",
    "          batch_size=128, epochs=1,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.save(\"lewis_clean_word.h5\") #loss = .0072, 225 epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
